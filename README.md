![Alt text](https://github.com/JordanMicahBennett/Supersymmetric-artificial-neural-network/blob/master/_image.png "default page")

Alternate-title [0]: “Thought Curvature”
============================================
“Thought Curvature” is similar to “[Thought vectors](https://en.wikipedia.org/wiki/Thought_vector)”, with the distinction that [supermanifold](https://en.wikipedia.org/wiki/Supermanifold)/[curvatures](https://en.wikipedia.org/wiki/Curvature) are used to describe the "Supersymmetric Artificial Neural Network"(SANN) model. (See [manifold](https://en.wikipedia.org/wiki/Manifold)/[curvature](https://en.wikipedia.org/wiki/Curvature) work in [geometric deep learning by Michael Bronstein et al.](https://arxiv.org/abs/1611.08097)

Alternate-title [1]: “Supersymmetric Gradient Descent”
============================================
In propagating small changes wrt some target space within a problem space, throughout the supersymmetric model, i.e. supersymmetric stochastic gradient descent. This is not to be confused for [Symmetric tensors](https://en.wikipedia.org/wiki/Symmetric_tensor) as seen in [old Higher Order Symmetric Tensor papers, that dont concern superspace, but falsely label said symmetric tensors as "supersymmetric tensors"](https://arxiv.org/pdf/1201.3424). Pertinently, [see this paper, describing the phenomena of "super" tensor labelling errors](https://arxiv.org/pdf/0802.1681). Notably, [even recent higher order tensor papers, that likewise dont concern superspace](https://arxiv.org/pdf/1701.0542) are still invalidly commiting the "super" labelling error, as described in the [error indicating paper prior cited](https://arxiv.org/pdf/0802.1681)).



What is the motivation behind the "Supersymmetric Artificial Neural Network?"
============================================

The "[Supersymmetric Artificial Neural Network](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis_-_on_the_%27Supersymmetric_Artificial_Neural_Network%27)" (or "**Edward Witten/String theory powered artificial neural network**") is a [Lie Superalgebra](https://en.wikipedia.org/wiki/Lie_superalgebra) aligned algorithmic learning model **(started/created by [myself](https://www.facebook.com/ProgrammingGodJordan) on May 10, 2016)**, based on [evidence ](https://arxiv.org/abs/0705.1134)pertaining to [Supersymmetry](https://en.wikipedia.org/wiki/Supersymmetry) in the biological brain.


To describe the **significance** of the "[Supersymmetric Artificial Neural Network](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis_-_on_the_%27Supersymmetric_Artificial_Neural_Network%27)", I will describe an **informal proof** of the **representation power** gained by **deeper abstractions** generatable by learning [supersymmetric](https://en.wikipedia.org/wiki/Supersymmetry) weights.

&nbsp;


![](https://i.imgur.com/0MF1WKV.jpg)


**Remember** that [Deep Learning](https://en.wikipedia.org/wiki/Deep_learning) is all about representation power, i.e. how much data the **artificial neural model** can **capture** from inputs, so as to produce **good guesses/hypotheses** about what the input data is talking about.

Machine learning is all about the application of families of functions that **guarantee more and more variations** in weight space.

&nbsp;

This means that machine learning researchers study what functions are best to transform the weights of the artificial neural network, such that the **weights learn** to represent **good values** for which **correct hypotheses or guesses** can be produced by the artificial neural network.

&nbsp;

**The** [**“Supersymmetric Artificial Neural Network”**](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis) is yet another way to **represent richer values** in the weights of the model; because **supersymmetric values** can allow for **more information to be captured about the input space.** For example, supersymmetric systems can capture **potential-partner** signals, which are **beyond** the feature space of **magnitude** and **phase signals** learnt in **typical** **real valued neural nets** and **deep complex neural networks** respectively. As such, a brief historical progression of geometric solution spaces for varying neural network architectures follows:

![](https://i.imgur.com/NRA0CH3.png)
paper: https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis

PAPER
==================
[https://www.researchgate.net/publication/Thought_Curvature](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis_-_on_the_%27Supersymmetric_Artificial_Neural_Network%27)

INVENTION PROCESS???
==================
1. I practice writing [basic artificial neural networks every 6 months or a year](https://github.com/JordanMicahBennett/NEURAL_NETWORK_PRACTICE), from scratch/memory.
2. I cross reference my implementations with [my understanding of basic artificial neural networks](https://www.researchgate.net/publication/321162382_Artificial_Neural_Nets_For_Kids).
3. I try to derive [mathematical overviews of the entire field](https://camo.githubusercontent.com/f241eca11ea6f241d264dcf9d8c0ae28ca2ecae6/68747470733a2f2f692e696d6775722e636f6d2f52366939414a632e706e67), to avoid only repitition of past studies/models. 

    i. Note that repitition tends to be beneficial for incremental improvements, such as incremental improvements found in [the various flavours of GAN](http://guimperarnau.com/blog/2017/03/Fantastic-GANs-and-where-to-find-them), created after [the original GAN](https://en.wikipedia.org/wiki/Generative_adversarial_network).
  
    ii. However, as far as I have researched, novel inventions require both repitition and largely new avenues of thought/research, such as [the original GAN](https://en.wikipedia.org/wiki/Generative_adversarial_network), and I venture to say, my [Supersymmetric Artificial Neural Network.](https://github.com/JordanMicahBennett/Supersymmetric-artificial-neural-networ)
    
4. I then apply math/machine learning literature to develop [a novel learning model](https://github.com/JordanMicahBennett/Supersymmetric-artificial-neural-network).

SUMMARY
==================

![](https://i.imgur.com/Uru3bU0.png)

YOUTUBE-SUMMARY
==================
https://www.youtube.com/watch?v=62YVT2LlXAI


THE SUPERSYMMETRIC ARTIFICIAIL NEURAL NETWORK STEMS FROM:
==================
https://jordanmicahbennett.github.io/Supermathematics-and-Artificial-General-Intelligence/

LET'S GET EVEN DEEPER!
==================
[Researchgate/Why is the purpose of human life to create Artificial General Intelligence?](https://www.researchgate.net/publication/319235750_Why_is_the_purpose_of_human_life_to_create_Artificial_General_Intelligence)


AUTHOR PORTFOLIO
============================================
http://folioverse.appspot.com/
